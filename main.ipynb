{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db5c019",
   "metadata": {},
   "source": [
    "# **On Calibration of Modern Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7644931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "from netcal.binning import HistogramBinning, BBQ\n",
    "from netcal.scaling import TemperatureScaling\n",
    "from netcal.metrics import ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c3bea268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_data(batch_size=64):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  \n",
    "        transforms.Lambda(lambda x: x.view(-1))  \n",
    "    ])\n",
    "\n",
    "    full_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # Divide treino completo em treino (60%) e validação (40%)\n",
    "    train_size = int(0.6 * len(full_train))  # 60% de 60000\n",
    "    val_size = len(full_train) - train_size  # 40% restante\n",
    "    train_data, val_data = random_split(full_train, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, device\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=128, output_dim=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# Criação dos dataloaders e modelo\n",
    "train_loader, val_loader, test_loader, device = create_mnist_data()\n",
    "model = SimpleNN().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c33c2",
   "metadata": {},
   "source": [
    "### **Vector Scaling // Matrix Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4a335a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorScaling(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(n_classes))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_classes))\n",
    "    def forward(self, logits):\n",
    "        return logits * self.scale + self.bias\n",
    "\n",
    "\n",
    "class MatrixScaling(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.eye(n_classes))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_classes))\n",
    "        \n",
    "    def forward(self, logits):\n",
    "        return torch.matmul(logits, self.weight) + self.bias\n",
    "\n",
    "\n",
    "class _ECELoss(nn.Module):\n",
    "    def __init__(self, n_bins=15):\n",
    "        super().__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            in_bin = (confidences > bin_lower) * (confidences <= bin_upper)\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e355fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 0.1030\n",
      "Epoch 10/20, Loss: 0.0905\n",
      "Epoch 15/20, Loss: 0.0622\n",
      "Epoch 20/20, Loss: 0.0649\n"
     ]
    }
   ],
   "source": [
    "def train_simple_model(train_loader, input_dim, n_classes, device, epochs=20, lr=0.01):\n",
    "    model = SimpleNN().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_simple_model(train_loader, 784, 10, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7f572",
   "metadata": {},
   "source": [
    "### **BBQ // Hist. Bins // Isotonic Regresiion // Temperature Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(probs, y_true, name):\n",
    "    acc = accuracy_score(y_true, np.argmax(probs, axis=1))\n",
    "    ece = ECE(15).measure(probs, y_true)\n",
    "    #print(f\"{name} - Accuracy: {acc:.4f}, ECE: {ece:.4f}\")\n",
    "    return [name, acc, ece]\n",
    "\n",
    "\n",
    "def train_nn(model, train_loader, val_loader, device, epochs=20, lr=0.01):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        # if (epoch + 1) % 5 == 0:\n",
    "            # print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Função para extrair probabilidades no conjunto de validação e teste\n",
    "    def get_probs(model, loader):\n",
    "        model.eval()\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.to(device)\n",
    "                logits = model(x)\n",
    "                probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "                all_probs.append(probs)\n",
    "                all_labels.append(y.numpy())\n",
    "        return np.vstack(all_probs), np.hstack(all_labels)\n",
    "    \n",
    "    probs_val, y_val = get_probs(model, val_loader)\n",
    "    probs_test, y_test = get_probs(model, test_loader)\n",
    "    \n",
    "    return model, probs_val, y_val, probs_test, y_test\n",
    "\n",
    "\n",
    "def calibrate_and_evaluate(probs_val, y_val, probs_test, y_test):\n",
    "    results = []\n",
    "    \n",
    "    results.append(evaluate(probs_test, y_test, \"Uncalibrated NN\"))\n",
    "    \n",
    "    hb = HistogramBinning()\n",
    "    hb.fit(probs_val, y_val)\n",
    "    results.append(evaluate(hb.transform(probs_test), y_test, \"Histogram Binning\"))\n",
    "    \n",
    "    iso_probs = []\n",
    "    for k in range(probs_val.shape[1]):\n",
    "        ir = IsotonicRegression(out_of_bounds='clip')\n",
    "        y_bin = (y_val == k).astype(int)\n",
    "        ir.fit(probs_val[:, k], y_bin)\n",
    "        iso_probs.append(ir.predict(probs_test[:, k]))\n",
    "    iso_probs = np.vstack(iso_probs).T\n",
    "    results.append(evaluate(iso_probs, y_test, \"Isotonic Regression\"))\n",
    "    \n",
    "    bbq = BBQ()\n",
    "    bbq.fit(probs_val, y_val)\n",
    "    results.append(evaluate(bbq.transform(probs_test), y_test, \"BBQ\"))\n",
    "    \n",
    "    ts = TemperatureScaling()\n",
    "    ts.fit(probs_val, y_val)\n",
    "    results.append(evaluate(ts.transform(probs_test), y_test, \"Temperature Scaling\"))\n",
    "    \n",
    "    return pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"ECE\"])\n",
    "\n",
    "model, probs_val, y_val, probs_test, y_test = train_nn(model, train_loader, val_loader, device, epochs=20, lr=0.01)\n",
    "\n",
    "results = calibrate_and_evaluate(probs_val, y_val, probs_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4c363b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithVectorScaling(nn.Module):\n",
    "    def __init__(self, model, n_classes):\n",
    "        super(ModelWithVectorScaling, self).__init__()\n",
    "        self.model = model\n",
    "        self.calibrator = VectorScaling(n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        calibrated_logits = self.calibrator(logits)\n",
    "        return calibrated_logits\n",
    "\n",
    "    def set_temperature(self, valid_loader, lr=0.01, max_iter=100):\n",
    "        device = next(self.parameters()).device\n",
    "        self.calibrator.train()\n",
    "        nll_criterion = nn.CrossEntropyLoss()\n",
    "        ece_criterion = _ECELoss()\n",
    "\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        with torch.no_grad():\n",
    "            for input, label in valid_loader:\n",
    "                input = input.to(device)\n",
    "                logits = self.model(input)\n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(label.to(device))\n",
    "            logits = torch.cat(logits_list)\n",
    "            labels = torch.cat(labels_list)\n",
    "\n",
    "        before_nll = nll_criterion(logits, labels).item()\n",
    "        before_ece = ece_criterion(logits, labels).item()\n",
    "        #print(f\"Before vector scaling - NLL: {before_nll:.3f}, ECE: {before_ece:.3f}\")\n",
    "\n",
    "        optimizer = optim.LBFGS(self.calibrator.parameters(), lr=lr, max_iter=max_iter)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll_criterion(self.calibrator(logits), labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        after_nll = nll_criterion(self.calibrator(logits), labels).item()\n",
    "        after_ece = ece_criterion(self.calibrator(logits), labels).item()\n",
    "        #print(f\"After vector scaling - NLL: {after_nll:.3f}, ECE: {after_ece:.3f}\")\n",
    "        return self\n",
    "    \n",
    "    \n",
    "class ModelWithMatrixScaling(nn.Module):\n",
    "    def __init__(self, model, n_classes):\n",
    "        super(ModelWithMatrixScaling, self).__init__()\n",
    "        self.model = model\n",
    "        self.calibrator = MatrixScaling(n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        calibrated_logits = self.calibrator(logits)\n",
    "        return calibrated_logits\n",
    "\n",
    "    def set_temperature(self, valid_loader, lr=0.01, max_iter=100):\n",
    "        device = next(self.parameters()).device\n",
    "        self.calibrator.train()\n",
    "        nll_criterion = nn.CrossEntropyLoss()\n",
    "        ece_criterion = _ECELoss()\n",
    "\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        with torch.no_grad():\n",
    "            for input, label in valid_loader:\n",
    "                input = input.to(device)\n",
    "                logits = self.model(input)\n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(label.to(device))\n",
    "            logits = torch.cat(logits_list)\n",
    "            labels = torch.cat(labels_list)\n",
    "\n",
    "        before_nll = nll_criterion(logits, labels).item()\n",
    "        before_ece = ece_criterion(logits, labels).item()\n",
    "        #print(f\"Before matrix scaling - NLL: {before_nll:.3f}, ECE: {before_ece:.3f}\")\n",
    "\n",
    "        optimizer = optim.LBFGS(self.calibrator.parameters(), lr=lr, max_iter=max_iter)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll_criterion(self.calibrator(logits), labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        after_nll = nll_criterion(self.calibrator(logits), labels).item()\n",
    "        after_ece = ece_criterion(self.calibrator(logits), labels).item()\n",
    "        #print(f\"After matrix scaling - NLL: {after_nll:.3f}, ECE: {after_ece:.3f}\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f03cc7",
   "metadata": {},
   "source": [
    "### **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fcc3af96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ECE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uncalibrated NN</td>\n",
       "      <td>0.9628</td>\n",
       "      <td>0.025151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Histogram Binning</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.005922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Isotonic Regression</td>\n",
       "      <td>0.9638</td>\n",
       "      <td>0.004706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BBQ</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.003576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Temperature Scaling</td>\n",
       "      <td>0.9628</td>\n",
       "      <td>0.025150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vector Scaling</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>0.012878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Matrix Scaling</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>0.013089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy       ECE\n",
       "0      Uncalibrated NN    0.9628  0.025151\n",
       "1    Histogram Binning    0.9630  0.005922\n",
       "2  Isotonic Regression    0.9638  0.004706\n",
       "3                  BBQ    0.9633  0.003576\n",
       "4  Temperature Scaling    0.9628  0.025150\n",
       "5       Vector Scaling    0.9613  0.012878\n",
       "6       Matrix Scaling    0.9613  0.013089"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_torch_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            all_logits.append(logits)\n",
    "            all_labels.append(y)\n",
    "    logits = torch.cat(all_logits)\n",
    "    labels = torch.cat(all_labels)\n",
    "\n",
    "    softmaxes = F.softmax(logits, dim=1).cpu().numpy()\n",
    "    preds = np.argmax(softmaxes, axis=1)\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    acc = (preds == labels_np).mean()\n",
    "\n",
    "    ece = _ECELoss()(logits, labels).item()\n",
    "    return acc, ece\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "model_ms = ModelWithMatrixScaling(model, n_classes).to(device)\n",
    "model_ms.set_temperature(val_loader)\n",
    "acc_ms, ece_ms = evaluate_torch_model(model_ms, test_loader, device)\n",
    "\n",
    "model_vs = ModelWithVectorScaling(model, n_classes).to(device)\n",
    "model_vs.set_temperature(val_loader)\n",
    "acc_vs, ece_vs = evaluate_torch_model(model_vs, test_loader, device)\n",
    "\n",
    "results.loc[len(results)] = [\"Vector Scaling\", acc_vs, ece_vs]\n",
    "results.loc[len(results)] = [\"Matrix Scaling\", acc_ms, ece_ms]\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
